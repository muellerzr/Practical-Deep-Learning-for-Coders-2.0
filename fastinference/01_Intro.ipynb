{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Introduction to `fastinference`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `fastinference`?\n",
    "\n",
    "  * Inference module for `fastai`\n",
    "  * Allows for faster inference, more verbose prediction methods, and some model interpretability modules\n",
    "  \n",
    "What will this tutorial cover? (NB 01 - n):\n",
    "  * We'll be looking at *why* these speedups work\n",
    "  * What they really are\n",
    "  * Other modules that are also packed away in `fastinference` such as `fastshap` and `ClassConfusion` among a few others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai fastinference --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Example\n",
    "\n",
    "First let's look at a tabular example (via ADULT_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(path/'adult.csv')\n",
    "splits = RandomSplitter()(range_of(df))\n",
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "y_names = 'salary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                   y_names=y_names, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = to.dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'constant',\n",
       " 'median',\n",
       " 'mode']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: FillMissing -> Categorify -> Categorize -> Normalize"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.procs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: ReadTabBatch"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.after_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a quick learner then check the speed of 2 items:\n",
    "\n",
    "* `learn.predict()`\n",
    "* `learn.get_preds()` on 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReadTabBatch: (object,object) -> encodes (object,object) -> decodes"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.after_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: FillMissing -> Categorify -> Categorize -> Normalize"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.procs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FillMissing??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, layers=[200,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for all the tests we will be having `CUDA` enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 83.9 ms, total: 280 ms\n",
      "Wall time: 280 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out = learn.predict(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = learn.dls.test_dl(df.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 ms, sys: 5.26 ms, total: 21.4 ms\n",
      "Wall time: 21.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out = learn.get_preds(dl=test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, a bit of time. Now next we'll look at a *raw* `PyTorch` inference loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat, cont, y = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520 µs ± 749 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    out = learn.model(cat, cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! Wait hold on. That's a *half* a *millisecond*! Why? \n",
    "\n",
    "Well, we haven't quite done everything the same yet. `predict` gives us probabilities and the class label and whatnot. So, let's recreate this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    out = learn.model(cat, cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to turn our probabilities into `argmax`'s, first applying a `softmax`. This comes from our `loss_func`'s `activation` and `decodes` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0088,  0.0212],\n",
       "        [ 0.0103,  0.0407],\n",
       "        [-0.0072,  0.0120],\n",
       "        [-0.0084,  0.0354],\n",
       "        [-0.0128,  0.0131]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4925, 0.5075],\n",
       "        [0.4924, 0.5076],\n",
       "        [0.4952, 0.5048],\n",
       "        [0.4890, 0.5110],\n",
       "        [0.4935, 0.5065]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.loss_func.activation(out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.loss_func.decodes(learn.loss_func.activation(out[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we skip the `activation`? Since it's a softmax, yes we can if we just want the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.loss_func.decodes(out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(   workclass  education  marital-status  occupation  relationship  race  \\\n",
       " 0        5.0        8.0             3.0         0.0           6.0   5.0   \n",
       " \n",
       "    education-num_na       age    fnlwgt  education-num  salary  \n",
       " 0               1.0  0.774021 -0.833759       0.753626     1.0  ,\n",
       " tensor(1),\n",
       " tensor([0.4925, 0.5075]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's time our new `predict` pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 µs ± 1.76 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "learn.model.eval()\n",
    "with torch.no_grad():\n",
    "    out = learn.model(cat, cont)\n",
    "raw_probs = learn.loss_func.activation(out)\n",
    "classes = learn.loss_func.decodes(raw_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still in `microseconds`. Why? How is it so much longer? The `Callback` system is the right answer. This is why I made `fastinference`. Let's import `fastinference` and see what it gives me:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastinference.inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.4 ms ± 35.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "out = learn.predict(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 ms ± 14 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dl = learn.dls.test_dl(df.iloc[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 10 milliseconds comes from building our `test_dl`, and we were able to cut our inference by 1/2 with `learn.predict`. What about `get_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.78 ms ± 14.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "preds = learn.get_preds(dl=test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also by almost half! But what else is different about these methods? Let's compare source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai\n",
    "def get_preds(self, ds_idx=1, dl=None, with_input=False, with_decoded=False, with_loss=False, act=None,\n",
    "                  inner=False, reorder=True, **kwargs):\n",
    "    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n",
    "    if reorder and hasattr(dl, 'get_idxs'):\n",
    "        idxs = dl.get_idxs()\n",
    "        dl = dl.new(get_idxs = _ConstantFunc(idxs))\n",
    "    cb = GatherPredsCallback(with_input=with_input, with_loss=with_loss, **kwargs)\n",
    "    ctx_mgrs = [self.no_logging(), self.added_cbs(cb), self.no_mbar()]\n",
    "    if with_loss: ctx_mgrs.append(self.loss_not_reduced())\n",
    "    with ExitStack() as stack:\n",
    "        for mgr in ctx_mgrs: stack.enter_context(mgr)\n",
    "        self(event.begin_epoch if inner else _before_epoch)\n",
    "        self._do_epoch_validate(dl=dl)\n",
    "        self(event.after_epoch if inner else _after_epoch)\n",
    "        if act is None: act = getattr(self.loss_func, 'activation', noop)\n",
    "        res = cb.all_tensors()\n",
    "        pred_i = 1 if with_input else 0\n",
    "        if res[pred_i] is not None:\n",
    "            res[pred_i] = act(res[pred_i])\n",
    "            if with_decoded: res.insert(pred_i+2, getattr(self.loss_func, 'decodes', noop)(res[pred_i]))\n",
    "        if reorder and hasattr(dl, 'get_idxs'): res = nested_reorder(res, tensor(idxs).argsort())\n",
    "        return tuple(res)\n",
    "    self._end_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty sophistcated. Why? It relies on the `Callback` system and `Schedulers`. What does mine do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine\n",
    "@patch\n",
    "def get_preds(x:Learner, ds_idx=1, dl=None, raw_outs=False, decoded_loss=True, fully_decoded=False,\n",
    "             **kwargs):\n",
    "    \"Get predictions with possible decoding\"\n",
    "    inps, outs, dec_out, raw = [], [], [], []\n",
    "    if dl is None: dl = x.dls[ds_idx].new(shuffle=False, drop_last=False)\n",
    "    is_multi=False\n",
    "    if x.dls.n_inp > 1:\n",
    "        is_multi=True\n",
    "        [inps.append([]) for _ in range(x.dls.n_inp)]\n",
    "    x.model.eval()\n",
    "    for batch in dl:\n",
    "        with torch.no_grad():\n",
    "            if is_multi:\n",
    "                for i in range(x.dls.n_inp):\n",
    "                    inps[i].append(batch[i])\n",
    "            else:\n",
    "                inps.append(batch[:x.dls.n_inp])\n",
    "            if decoded_loss or fully_decoded:\n",
    "                out = x.model(*batch[:x.dls.n_inp])\n",
    "                raw.append(out)\n",
    "                dec_out.append(x.loss_func.decodes(out))\n",
    "            else:\n",
    "                raw.append(x.model(*batch[:x.dls.n_inp]))\n",
    "    raw = torch.cat(raw, dim=0).cpu().numpy()\n",
    "    if fully_decoded or decoded_loss:\n",
    "        dec_out = torch.cat(dec_out, dim=0)\n",
    "    if not raw_outs:\n",
    "        try: outs.insert(0, x.loss_func.activation(tensor(raw)).numpy())\n",
    "        except: outs.insert(0, dec_out)\n",
    "    else:\n",
    "        outs.insert(0, raw)\n",
    "    if fully_decoded: outs = _fully_decode(x.dls, inps, outs, dec_out, is_multi)\n",
    "    if decoded_loss: outs = _decode_loss(x.dls.vocab, dec_out, outs)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so complicated now, is it? Let's do one more time, setting everything to `False` except `raw`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.52 ms ± 76.6 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = learn.get_preds(dl=test_dl, raw_outs=True, decoded_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now let's take a look inside `get_preds` and `predict`, specifically what they output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, preds, inp = learn.predict(df.iloc[0], with_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['>=50k'], array([[0.46804622, 0.5319538 ]], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>education-num_na</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>False</td>\n",
       "      <td>49.0</td>\n",
       "      <td>101320.00122</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see we get the class name (no longer just the class index), the probabilities, and the actual input (if `with_input=True`). What about `get_preds`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, preds = learn.get_preds(dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['>=50k', '>=50k', '>=50k'],\n",
       " array([[0.46804622, 0.5319538 ],\n",
       "        [0.4700674 , 0.52993256],\n",
       "        [0.4769747 , 0.5230253 ]], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[:3], preds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing, however we have a bit more parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(x:Learner, ds_idx=1, dl=None, raw_outs=False, decoded_loss=True, fully_decoded=False,\n",
    "             **kwargs):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what all does this mean? `raw_outs` applies our `softmax` or not, `decoded_loss` does our `argmax`, and `fully_decoded` returns our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, preds = learn.get_preds(dl=test_dl, raw_outs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['>=50k', '>=50k', '>=50k'],\n",
       " array([[-0.08579029,  0.04219937],\n",
       "        [-0.05968028,  0.06019334],\n",
       "        [-0.0669051 ,  0.02526121]], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[:3], preds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.get_preds(dl=test_dl, decoded_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46804622, 0.5319538 ],\n",
       "       [0.4700674 , 0.52993256],\n",
       "       [0.4769747 , 0.5230253 ]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, preds, inp = learn.get_preds(dl=test_dl, decoded_loss=True, fully_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['>=50k', '>=50k', '>=50k'],\n",
       " array([[0.46804622, 0.5319538 ],\n",
       "        [0.4700674 , 0.52993256],\n",
       "        [0.4769747 , 0.5230253 ]], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[:3], preds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>education-num_na</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.758773</td>\n",
       "      <td>-0.840070</td>\n",
       "      <td>0.755614</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.392702</td>\n",
       "      <td>0.444001</td>\n",
       "      <td>1.540485</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.046583</td>\n",
       "      <td>-0.888759</td>\n",
       "      <td>-0.029257</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        5.0        8.0             3.0         0.0           6.0   5.0   \n",
       "1        5.0       13.0             1.0         5.0           2.0   5.0   \n",
       "2        5.0       12.0             1.0         0.0           5.0   3.0   \n",
       "\n",
       "   education-num_na       age    fnlwgt  education-num  salary  \n",
       "0               1.0  0.758773 -0.840070       0.755614     1.0  \n",
       "1               1.0  0.392702  0.444001       1.540485     1.0  \n",
       "2               2.0 -0.046583 -0.888759      -0.029257     1.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also work for any other application in the `fastai` library, int eh same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
