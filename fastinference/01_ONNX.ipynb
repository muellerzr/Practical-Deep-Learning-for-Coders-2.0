{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX\n",
    "\n",
    "Commonly code is run on ONNX, and trying to utilize `fastai` as best you can can be a challenge. The beneifits of ONNX is it uses C++ so it can be a faster runtime, and recently ONNX came out with `CUDA` support as well! How hard is it to integrate? Let's make a `tabular` problem again, this time importing `ONNX`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastinference\r\n",
      "Version: 0.0.13\r\n",
      "Summary: A collection of inference modules\r\n",
      "Home-page: https://github.com/muellerzr/fastinference/tree/master/\r\n",
      "Author: Zachary Mueller\r\n",
      "Author-email: muellerzr@gmail.com\r\n",
      "License: Apache Software License 2.0\r\n",
      "Location: /home/ml1/anaconda3/envs/fastai/lib/python3.7/site-packages\r\n",
      "Requires: onnxruntime-gpu, fastai, shap\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show fastinference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "from fastinference.onnx import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(path/'adult.csv')\n",
    "splits = RandomSplitter()(range_of(df))\n",
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "y_names = 'salary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                   y_names=y_names, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = to.dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, layers=[200,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've made a special `fastONNX` wrapper, which will take your `Learner` and export both your model and the `DataLoaders` so ONNX can use them via `learn.to_onnx()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.to_onnx('tabular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load it in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_learn = fastONNX('tabular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What all can we do here? We can still do everything exactly the same minus one change: `predict`\n",
    "\n",
    "`predict` requires the raw inputs, so instead *always* build a `test_dl` and pass to `get_preds` (just made it simpler to code for me). Let's run a few examples also testing the times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dl = onnx_learn.test_dl(df.iloc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.46 ms ± 5.44 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "preds = onnx_learn.get_preds(dl=single_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! 3.46 ms even beats our previous `predict` with 25 ms! what is it's output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, preds = onnx_learn.get_preds(dl=single_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<50k'], array([[[0.5036924, 0.4963076]]], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently it doesn't support returning the raw inputs, I need to work on that some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
